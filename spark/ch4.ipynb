{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"my second app\")\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1.collect() = ['key1 word1', 'key2 word2', '']\n",
      "pairs.collect() = [('key1', 'key1 word1'), ('key2', 'key2 word2'), ('', '')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"myTextFile.txt\")\n",
    "print(\"rdd1.collect() =\", rdd1.collect())\n",
    "pairs = rdd1.map(lambda x: (x.split(\" \")[0], x))\n",
    "print(\"pairs.collect() =\", pairs.collect())\n",
    "print(\"123\"[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exampleRdd.collect() = [[1, 2], [3, 4], [3, 6], [3, 7]]\n",
      "examplePairs.collect() = [(1, 2), (3, 4), (3, 6), (3, 7)]\n"
     ]
    }
   ],
   "source": [
    "exampleRdd = sc.parallelize([[1, 2], [3, 4], [3, 6], [3, 7]])\n",
    "print(\"exampleRdd.collect() =\", exampleRdd.collect())\n",
    "examplePairs = exampleRdd.map(lambda x: (x[0], x[1]))\n",
    "print(\"examplePairs.collect() =\", examplePairs.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd.collect() = [(1, 2), (3, 17)]\n"
     ]
    }
   ],
   "source": [
    "newRdd = examplePairs.reduceByKey(lambda x, y: x + y)\n",
    "print(\"newRdd.collect() =\", newRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd2.collect() = [(1, <pyspark.resultiterable.ResultIterable object at 0x0101C590>), (3, <pyspark.resultiterable.ResultIterable object at 0x0101C5B0>)]\n",
      "groupedRdd.collect() = [(1, [2]), (3, [4, 6, 7])]\n"
     ]
    }
   ],
   "source": [
    "newRdd2 = examplePairs.groupByKey()\n",
    "print(\"newRdd2.collect() =\", newRdd2.collect())\n",
    "groupedRdd = newRdd2.map(lambda x: (x[0], list(x[1])))\n",
    "print(\"groupedRdd.collect() =\", groupedRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd3.collect() = [(1, 3), (3, 5), (3, 7), (3, 8)]\n"
     ]
    }
   ],
   "source": [
    "newRdd3 = examplePairs.mapValues(lambda x: x + 1)\n",
    "print(\"newRdd3.collect() =\", newRdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd4.collect() = [(1, 3), (3, 5), (3, 7), (3, 8)]\n"
     ]
    }
   ],
   "source": [
    "newRdd4 = examplePairs.map(lambda x: (x[0], x[1] + 1))\n",
    "print(\"newRdd4.collect() =\", newRdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd5.collect() = [1, 3, 3, 5, 3, 7, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd5 = examplePairs.flatMap(lambda x: (x[0], x[1] + 1))\n",
    "print(\"newRdd5.collect() =\", newRdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd6.collect() = [1, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd6 = examplePairs.keys()\n",
    "print(\"newRdd6.collect() =\", newRdd6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mySet = {1, 3}\n",
      "myList = [1, 3]\n"
     ]
    }
   ],
   "source": [
    "# now I want to suppress duplicates\n",
    "listWithDuplicates = [1, 3, 3, 3]\n",
    "mySet = set(listWithDuplicates)\n",
    "print(\"mySet =\", mySet)\n",
    "myList = list(mySet)\n",
    "print(\"myList =\", myList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd7.collect() = [2, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd7 = examplePairs.values()\n",
    "print(\"newRdd7.collect() =\", newRdd7.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsortedRdd.collect() = [[1, 1], [3, 3], [2, 2]]\n",
      "unsortedPairsRdd.collect() = [(1, 1), (3, 3), (2, 2)]\n",
      "sortedPairsRdd.collect() = [(1, 1), (2, 2), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "unsortedKeyValueList = [[1,1], [3,3], [2,2] ]\n",
    "unsortedRdd = sc.parallelize(unsortedKeyValueList)\n",
    "print(\"unsortedRdd.collect() =\", unsortedRdd.collect())\n",
    "unsortedPairsRdd = unsortedRdd.map(lambda x: (x[0], x[1]))\n",
    "print(\"unsortedPairsRdd.collect() =\", unsortedPairsRdd.collect())\n",
    "sortedPairsRdd = unsortedPairsRdd.sortByKey()\n",
    "print(\"sortedPairsRdd.collect() =\", sortedPairsRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtractedRdd.collect() = [(1, 1), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [[1,1], [3,3], [2,2]]\n",
    "list2 = [[4,4], [2,22]]\n",
    "rdd1 = sc.parallelize(list1)\n",
    "rdd2 = sc.parallelize(list2)\n",
    "pairsRdd1 = rdd1.map(lambda x: (x[0], x[1]))\n",
    "pairsRdd2 = rdd2.map(lambda x: (x[0], x[1]))\n",
    "subtractedRdd = pairsRdd1.subtractByKey(pairsRdd2)\n",
    "print(\"subtractedRdd.collect() =\", subtractedRdd.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joinedRdd.collect() = [(2, (2, 22))]\n"
     ]
    }
   ],
   "source": [
    "joinedRdd = pairsRdd1.join(pairsRdd2)\n",
    "print(\"joinedRdd.collect() =\", joinedRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rightOuterJoinRdd.collect() = [(2, (2, 22)), (4, (None, 4))]\n",
      "leftOuterJoinRdd.collect() = [(2, (2, 22)), (1, (1, None)), (3, (3, None))]\n"
     ]
    }
   ],
   "source": [
    "rightOuterJoinRdd = pairsRdd1.rightOuterJoin(pairsRdd2)\n",
    "print(\"rightOuterJoinRdd.collect() =\", rightOuterJoinRdd.collect())\n",
    "leftOuterJoinRdd = pairsRdd1.leftOuterJoin(pairsRdd2)\n",
    "print(\"leftOuterJoinRdd.collect() =\", leftOuterJoinRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairsRdd1.collect() = [(1, 1), (3, 3), (2, 2)]\n",
      "pairsRdd2.collect() = [(4, 4), (2, 22)]\n",
      "cogroupRdd.collect() = [(2, (<pyspark.resultiterable.ResultIterable object at 0x0101C750>, <pyspark.resultiterable.ResultIterable object at 0x0101CC30>)), (4, (<pyspark.resultiterable.ResultIterable object at 0x0101CFF0>, <pyspark.resultiterable.ResultIterable object at 0x0101C1D0>)), (1, (<pyspark.resultiterable.ResultIterable object at 0x0101C830>, <pyspark.resultiterable.ResultIterable object at 0x0101C670>)), (3, (<pyspark.resultiterable.ResultIterable object at 0x0101C510>, <pyspark.resultiterable.ResultIterable object at 0x0101C270>))]\n"
     ]
    }
   ],
   "source": [
    "print(\"pairsRdd1.collect() =\", pairsRdd1.collect())\n",
    "print(\"pairsRdd2.collect() =\", pairsRdd2.collect())\n",
    "\n",
    "cogroupRdd = pairsRdd1.cogroup(pairsRdd2)\n",
    "print(\"cogroupRdd.collect() =\", cogroupRdd.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
