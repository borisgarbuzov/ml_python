{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"my second app\")\n",
    "sc = SparkContext(conf = conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd1.collect() = ['key1 word1', 'key2 word2', '']\n",
      "pairs.collect() = [('key1', 'key1 word1'), ('key2', 'key2 word2'), ('', '')]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.textFile(\"myTextFile.txt\")\n",
    "print(\"rdd1.collect() =\", rdd1.collect())\n",
    "pairs = rdd1.map(lambda x: (x.split(\" \")[0], x))\n",
    "print(\"pairs.collect() =\", pairs.collect())\n",
    "print(\"123\"[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exampleRdd.collect() = [[1, 2], [3, 4], [3, 6], [3, 7]]\n",
      "examplePairs.collect() = [(1, 2), (3, 4), (3, 6), (3, 7)]\n"
     ]
    }
   ],
   "source": [
    "exampleRdd = sc.parallelize([[1, 2], [3, 4], [3, 6], [3, 7]])\n",
    "print(\"exampleRdd.collect() =\", exampleRdd.collect())\n",
    "examplePairs = exampleRdd.map(lambda x: (x[0], x[1]))\n",
    "print(\"examplePairs.collect() =\", examplePairs.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd.collect() = [(1, 2), (3, 17)]\n"
     ]
    }
   ],
   "source": [
    "newRdd = examplePairs.reduceByKey(lambda x, y: x + y)\n",
    "print(\"newRdd.collect() =\", newRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd2.collect() = [(1, <pyspark.resultiterable.ResultIterable object at 0x0085E450>), (3, <pyspark.resultiterable.ResultIterable object at 0x0085E470>)]\n",
      "groupedRdd.collect() = [(1, [2]), (3, [4, 6, 7])]\n"
     ]
    }
   ],
   "source": [
    "newRdd2 = examplePairs.groupByKey()\n",
    "print(\"newRdd2.collect() =\", newRdd2.collect())\n",
    "groupedRdd = newRdd2.map(lambda x: (x[0], list(x[1])))\n",
    "print(\"groupedRdd.collect() =\", groupedRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd3.collect() = [(1, 3), (3, 5), (3, 7), (3, 8)]\n"
     ]
    }
   ],
   "source": [
    "newRdd3 = examplePairs.mapValues(lambda x: x + 1)\n",
    "print(\"newRdd3.collect() =\", newRdd3.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newRdd4.collect() = [(1, 3), (3, 5), (3, 7), (3, 8)]\n"
     ]
    }
   ],
   "source": [
    "newRdd4 = examplePairs.map(lambda x: (x[0], x[1] + 1))\n",
    "print(\"newRdd4.collect() =\", newRdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd5.collect() = [1, 3, 3, 5, 3, 7, 3, 8]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd5 = examplePairs.flatMap(lambda x: (x[0], x[1] + 1))\n",
    "print(\"newRdd5.collect() =\", newRdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd6.collect() = [1, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd6 = examplePairs.keys()\n",
    "print(\"newRdd6.collect() =\", newRdd6.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mySet = {1, 3}\n",
      "myList = [1, 3]\n"
     ]
    }
   ],
   "source": [
    "# now I want to suppress duplicates\n",
    "listWithDuplicates = [1, 3, 3, 3]\n",
    "mySet = set(listWithDuplicates)\n",
    "print(\"mySet =\", mySet)\n",
    "myList = list(mySet)\n",
    "print(\"myList =\", myList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examplePairs.collect(): [(1, 2), (3, 4), (3, 6), (3, 7)]\n",
      "newRdd7.collect() = [2, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "print(\"examplePairs.collect():\", examplePairs.collect())\n",
    "newRdd7 = examplePairs.values()\n",
    "print(\"newRdd7.collect() =\", newRdd7.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsortedRdd.collect() = [[1, 1], [3, 3], [2, 2]]\n",
      "unsortedPairsRdd.collect() = [(1, 1), (3, 3), (2, 2)]\n",
      "sortedPairsRdd.collect() = [(1, 1), (2, 2), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "unsortedKeyValueList = [[1,1], [3,3], [2,2] ]\n",
    "unsortedRdd = sc.parallelize(unsortedKeyValueList)\n",
    "print(\"unsortedRdd.collect() =\", unsortedRdd.collect())\n",
    "unsortedPairsRdd = unsortedRdd.map(lambda x: (x[0], x[1]))\n",
    "print(\"unsortedPairsRdd.collect() =\", unsortedPairsRdd.collect())\n",
    "sortedPairsRdd = unsortedPairsRdd.sortByKey()\n",
    "print(\"sortedPairsRdd.collect() =\", sortedPairsRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtractedRdd.collect() = [(1, 1), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [[1,1], [3,3], [2,2]]\n",
    "list2 = [[4,4], [2,22]]\n",
    "rdd1 = sc.parallelize(list1)\n",
    "rdd2 = sc.parallelize(list2)\n",
    "pairsRdd1 = rdd1.map(lambda x: (x[0], x[1]))\n",
    "pairsRdd2 = rdd2.map(lambda x: (x[0], x[1]))\n",
    "subtractedRdd = pairsRdd1.subtractByKey(pairsRdd2)\n",
    "print(\"subtractedRdd.collect() =\", subtractedRdd.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joinedRdd.collect() = [(2, (2, 22))]\n"
     ]
    }
   ],
   "source": [
    "joinedRdd = pairsRdd1.join(pairsRdd2)\n",
    "print(\"joinedRdd.collect() =\", joinedRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rightOuterJoinRdd.collect() = [(2, (2, 22)), (4, (None, 4))]\n",
      "leftOuterJoinRdd.collect() = [(2, (2, 22)), (1, (1, None)), (3, (3, None))]\n"
     ]
    }
   ],
   "source": [
    "rightOuterJoinRdd = pairsRdd1.rightOuterJoin(pairsRdd2)\n",
    "print(\"rightOuterJoinRdd.collect() =\", rightOuterJoinRdd.collect())\n",
    "leftOuterJoinRdd = pairsRdd1.leftOuterJoin(pairsRdd2)\n",
    "print(\"leftOuterJoinRdd.collect() =\", leftOuterJoinRdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairsRdd1.collect() = [(1, 1), (3, 3), (2, 2)]\n",
      "pairsRdd2.collect() = [(4, 4), (2, 22)]\n",
      "cogroupRdd.collect() = [(2, (<pyspark.resultiterable.ResultIterable object at 0x00846F90>, <pyspark.resultiterable.ResultIterable object at 0x00846BD0>)), (4, (<pyspark.resultiterable.ResultIterable object at 0x00846ED0>, <pyspark.resultiterable.ResultIterable object at 0x00846AD0>)), (1, (<pyspark.resultiterable.ResultIterable object at 0x0A4B1030>, <pyspark.resultiterable.ResultIterable object at 0x008469F0>)), (3, (<pyspark.resultiterable.ResultIterable object at 0x00884970>, <pyspark.resultiterable.ResultIterable object at 0x008843B0>))]\n",
      "[(2, ([2], [22])), (4, ([], [4])), (1, ([1], [])), (3, ([3], []))]\n"
     ]
    }
   ],
   "source": [
    "print(\"pairsRdd1.collect() =\", pairsRdd1.collect())\n",
    "print(\"pairsRdd2.collect() =\", pairsRdd2.collect())\n",
    "cogroupRdd = pairsRdd1.cogroup(pairsRdd2)\n",
    "print(\"cogroupRdd.collect() =\", cogroupRdd.collect())\n",
    "\n",
    "complicated_construction = [(x, tuple(map(list, y))) for x, y in list(cogroupRdd.collect())]\n",
    "print(complicated_construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair =  (1, [[11, 12], [13, 14]])\n",
      "key =  1 , value = [[11, 12], [13, 14]]\n",
      "pair =  (2, [[22, 23], [24, 25]])\n",
      "key =  2 , value = [[22, 23], [24, 25]]\n",
      "pair =  (3, [[33, 34], [35, 36]])\n",
      "key =  3 , value = [[33, 34], [35, 36]]\n",
      "newList = [(1, [[11, 12], [13, 14]]), (2, [[22, 23], [24, 25]]), (3, [[33, 34], [35, 36]])]\n",
      "simpleList = [1, 2, 3]\n",
      "simpleTuple = (1, 2, 3)\n",
      "listAgain = [1, 2, 3]\n",
      "coordinate1 = 1 coordinate2 = 2 coordinate3 = 3\n",
      "type(simpleSimpleTuple) = <class 'tuple'>\n",
      "simpleSimpleTuple = (1, 2, 3)\n",
      "myMapList = <map object at 0x00DE7890>\n",
      "listPlus1 = [2, 3, 4]\n",
      "listSum = [123, 246]\n",
      "myGeneratorList = [1, 4]\n"
     ]
    }
   ],
   "source": [
    "# [(x, tuple(map(list, y))) for x, y in list(cogroupRdd.collect())]\n",
    "myPairList = [(1, [[11, 12], [13, 14]]), (2, [[22, 23], [24, 25]]), (3, [[33, 34], [35, 36]])]\n",
    "newList = []\n",
    "for pair in myPairList:\n",
    "    print (\"pair = \", pair)\n",
    "    key, value = pair[0], pair[1]\n",
    "    print (\"key = \", key, \", value =\", value)\n",
    "    newList.append((key, value))\n",
    "print(\"newList =\", newList)\n",
    "simpleList = [1, 2, 3]\n",
    "print(\"simpleList =\", simpleList)\n",
    "simpleTuple = tuple(simpleList)\n",
    "print(\"simpleTuple =\", simpleTuple)\n",
    "listAgain = list(simpleTuple)\n",
    "print(\"listAgain =\", listAgain)\n",
    "(coordinate1, coordinate2, coordinate3) = simpleTuple\n",
    "print(\"coordinate1 =\", coordinate1, \"coordinate2 =\", coordinate2, \"coordinate3 =\", coordinate3)\n",
    "simpleSimpleTuple = (1, 2, 3)\n",
    "print(\"type(simpleSimpleTuple) =\", type(simpleSimpleTuple))\n",
    "\n",
    "print(\"simpleSimpleTuple =\", simpleSimpleTuple)\n",
    "# map function\n",
    "# instead of list function use map, to make a list out of tuple\n",
    "myMapList = map(list, simpleSimpleTuple)\n",
    "print(\"myMapList =\", myMapList)\n",
    "# the above needs adjustment\n",
    "simpleList = [1, 2, 3]\n",
    "listPlus1 = list(map(lambda x: x+1, simpleList))\n",
    "print(\"listPlus1 =\", listPlus1)\n",
    "# may be in a new implementation they changed it, and they return the object map\n",
    "onePlusTwo = list(map(lambda x, y, z: x+y+z, [1, 2, 3], [11, 22], [111, 222, 333, 444]))\n",
    "print(\"listSum =\", onePlusTwo)\n",
    "myGeneratorList = [x**2 for x in [1, 2]]\n",
    "print(\"myGeneratorList =\", myGeneratorList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
